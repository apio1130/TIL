# 스터디 메

실습 교재: 카프카 스트림즈와 ksqlDB 정복

# 1장 요약
아파치 카프카는 분산 환경에 배치돼 있고 추가-전용 로그를 통해메시지를 생산하고 소비하는 것이 핵심.

1.카프카 설치
- docker-compose.yml 파일 생성
- `docker-compose up` 명령어 실행

2.카프카 실습
```
-- 카프카가 설치되어 있는 컨테이너에 로그인
docker-compose exec kafka bash

-- 토픽 생성
kafka-topics --bootstrap-server localhost:9092 --create --topic users --partitions 4 --replication-factor 1

-- 토픽 확인
kafka-topics --bootstrap-server localhost:9092 --describe --topic users

-- 메시지 퍼블리싱
kafka-console-producer --bootstrap-server localhost:9092 --property key.separator=, --property parse.key=true --topic users

>1,mitch
>2,elyse
>3,isabelle
>4,sammy

-- 메시지 컨슘
kafka-console-consumer --bootstrap-server localhost:9092 --topic users --from-beginning

mitch
isabelle
sammy
elyse

-- 메시지의 타임스탬프, 헤더, 키, 값 확인
kafka-console-consumer --bootstrap-server localhost:9092 --topic users --property print.timestamp=true --property print.key=true --property print.value=true --property print.headers=true --from-beginning

CreateTime:1680532754637        NO_HEADERS      1       mitch
CreateTime:1680532767095        NO_HEADERS      3       isabelle
CreateTime:1680532771303        NO_HEADERS      4       sammy
CreateTime:1680532761944        NO_HEADERS      2       elyse
```

3.실습종료
- `docker-compose down`

# 2장 카프카 스트림즈 시작하기
## 카프카 생태계
카프카 스트림즈는 카프카 생태계라 불리는 기술 그룹 내에 존재.

카프카 데이터 이동 관점에서 카프카 생태계 API 요약
```
1. Producer API
    - 카프카 토픽으로 메시지를 송신
    - 사용 예시: Filebeat, rsyslog, 커스텀 프로듀서
1. Consumer API
    - 카프카로부터 메시지를 수신
    - 사용 예시: kafkacat, 커스텀 컨슈머
1. Connect API
    - 외부 데이터 저장소, API, 파일 시스템을 카프카 토픽과 연결
    - 토픽으로부터 데이터를 읽거나 토픽으로 데이터를 내보냄(싱크 커넥터)
    - 사용 예시: JDBC 소스 커넥터, 엘라스틱 서치 싱크 커넥터, 커스텀 커넥터
```

### 카프카 스트림즈 이전
카프카 기반 스트림 처리 애플리케이션 구축에 두 가지 선택 사항이 존재.

두 가지 선택사항
1. Consumer 와 Producer API를 직접 사용
1. 별도 스트림 처리 프레임워크 사용(예: 아파치 스파크 스트리밍, 아파치 플링크)

1번 API 직접 사용 시 스트림 처리 API로서 갖춰야 할 기본적인 요소가 많이 부족함
- 로컬 내결함성 상태
- 데이터 스트림을 변환할 다양한 연산자들
- 고급 스트림 표현
- 정교한 시간 처리

2번 별도 스트림 처리 사용의 문제
- 불필요하게 많은 복잡성


### 카프카 스트림즈 이후
- 카프카 커뮤니티에서 스트림 처리 API의 필요성을 인지하고 카프카 스트림즈를 만들기로 결정.
- 2016년 카프카 스트림즈 첫 번째 버전이 릴리스.
- 초기 수작업에 크게 의존했던 스트림 처리 애플리케이션 개발에서 실시간 이벤트 스트림 변환, 
처리 패턴과 추가상화를 이용해 좀 더 고급스런 애플리케이션을 개발할 수 있도록 바뀜.
- 카프카 스트림즈는 Producer, Consumer와 Connect API와 달리
카프카로 혹은 카프카로부터 단순히 데이터만 이동시키는 것이 아닌, 실시간 데이터 스트림 처리에 특화되어 있음.

## 한눈에 보는 카프카 스트림즈 특징
- 자바 스트리밍 API처럼 느껴지는 상위-수준의 DSL(Domain Specific Language: 도메인 특화 언어)
    - 데이터 스트림 처리에 특화되어 있으며, 함수적인 접근을 제공하며 쉽게 배워 사용할 수 있음
- 세세한 제어가 필요할 때 사용하는 하위-수준의 Processor API
- 스트림이나 테이블처럼 데이터를 모델링해주는 편리한 추상화
- 데이터 변환과 보강에 유용한 스트림과 테이블의 조인 기능
- 상태가 없거나 상태가 있는 모든 스트림 처리 애플리케이션 구축에 사용 가능한 연산자와 도구들
- 윈도우 처리와 주기적인 함수 등을 포함하는 시간-기반 연산 지원
- 쉬운 설치
- 확장성, 신뢰성, 유지보수성

## 운영 특성
확장성(Saclability)
신뢰성(Reliability)
유지보수성(Maintainability)

## 다른 시스템과 비교
배치 모델(deploy model)
- 카프카 스트림즈는 자바 라이브러리로 구현됨
- 클러스터 관리가 필요 없어 처음 시작할 때 다른 프레임워크 보다 훨씬 쉬움
- 단순히 독립형 프로그램으로서 자바 애플리케이션에 카프카 스트림즈 의존 라이브러리만 추가하면 됨
    - 모니터링, 패키징, 코드 배치에 있어 자유도가 높음
- 기존 시스템과의 빠른 통합 능력

처리 모델
- 카프카 스트림즈는 한 번에 한 이벤트 방식(event-at-a-time)으로 구현됨
    - 이벤트가 들어오자마자 한 번에 하나씩 즉시 처리됨
    - 마이크로-배치라 하는 다른 접근 방식에 비해 낮은 지연 시간을 제공

카파 아키텍처(kappa)
- 카프카 스트림즈는 스트리밍 처리에만 초점을 맞추고 있음
    - 이를 카파 아키텍처라 부름
- 아파치 플링크나 아파치 스파크는 일괄 처리와 스트림처리 모두를 지원
    - 이를 람다 아키텍처라 부름


## 카프카 스트림즈 적용 사례들
1. 금융 데이터 처리, 구매 모니터링, 사기 감지
1. 알고리듬 거래
1. 주식 시장/암호 화폐 거래 모니터링
1. 실시간 재고 추적과 보충
1. 이메일 전달 추적과 모니터링
1. 비디오 게임 원격 측정 처리
1. 검색 색인
1. 지리 공간 추적/계산
1. 스마트 홈/IOT 센서 처리
1. 변경 데이터 갭처
1. 스포츠 방송/실시간 위젯
1. 실시간 광고 플랫폼
1. 의료 예측, 생명 유지 장치 모니터링
1. 챗 인프라, 챗봇, 가상 비서
1. 머신 러닝 파이프라인과 플랫폼
